### perceptron

ann 의 기본 구조 

y = NonLinear(xTw)

위의 모델은 bias를 넣지 않은 모델이야. 이 경우 반드시 원점을 지나야 하는 한계가 있어.

그래서 수정된 구조

y = sigma(xTw + w0)

---

### loss function

그러면 이러한 가중치는 어떻게 변경할까?

역전파 방식을 이용해서 변경한는데,

역전파는 실제값과 예측값의 차이를 통해 경사하강법으로 가중치를 변경하는 방식이야.

그래서 우선 값의 차이를 알기위해 우리는 손실함수를 정해야해.

보통 크게 두 종류의 손실함수가 있어.

#### Mean-squared error (MSE)

연속데이터를 사용하면서 출력도 연속된 값일때.

L = 1/2 * (y_pred - y)^2

#### Cross-entropy (logistic error)

범주형 데이터를 써서 확률로 나타내는 함수
L = -(ylog(y_pred) + (1-y)log(1-y_pred))


### Cost function

이때 손실함수는 한 샘플링의 차이를 의미해. 

여기서 손실함수의 값의 평균을 구한것이 비용함수야.

J = 1/n * sum(L(y_pred_i,y_i)) = 1/n*sum(L(f(x,W)_i, y_i))

이러한 비용함수를 경사하강법을 적용할 기준 함수로 사용을 해.

#### 그럼 왜 손실함수가 아닌 비용함수가 기준이 되는가?

- 보든 샘플링마다 손실을 구하는 건 엄청난 시간적 과부하가 있고, 과적합을 야기할 수 있어.
- 그런데 여기서 너무 많은 샘플의 평균을 구하게 될 경우 민감도가 감소할 수 있어. 특히 표본 변동성이 클 경우
- 그래서 가장 좋은 방법은 샘플링 단위가 아니라 batch 단위로 비용함수를 구하는 거야. (미니배치경사하강법)

---

### back propergation

일단 이제부터 하나의 퍼셉트론을 unit(node) 으로 표시할거야.

그리고 이 노드들은 모두 독립적이야. 전체의 상황을 모르는 상태야.

이 때, 역전파는 완전히 경사하강법과 동일하게 이루어져.

그런데 우리가 바꿀 수 있는건 w 뿐이라.

w <- w - lr*Loss_deriv (vanila gradient descent)

Loss_deriv = deriv(L(non_linear(xTw,y))

이때 Loss는 Cost로 변경되어 쓸 수 있어. 
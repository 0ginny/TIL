딥러닝의 작동 순서

1. 임의의 값으로 예측을 시작해
2. 실제 값과 얼마나 차이가 있는지 측정해.
3. 손실을 통해 가중치를 변경해.

여기서 3 번의 과정이 가장 어려운데, 이 3번 과정을 하기 위해 경사하강법을 사용해.

간단하게는 랜덤 x 로 y 값을 얻어, 그 위치에서 x-y' 한 값으로 다시 계산하고 반복하는 거야.

추가적으로 여기에 x-lr*y' 처럼 학습률을 곱해줘서 사용해.

이것은 완전 정확한 극소값을 얻진 못해도, 한가지를 보장해. 바로 경사가 낮아지는 곳으로 이동한다는 거지.

---

그러나 만약 이미 극값에 있다면? 움직이지 않을 것이고, 극소값이 최소값이 아닐 경우도 있어.

그런데 이것이 의미가 있는 이유는 ,딥러닝은 아주 높은 차원의 수를 다뤄.

그래서 그 모든 차원에서 극소값은 매우매우 적을 거라는 거지. 어쩌면 없을 수도 있어.

정리하자면, 이런 경사하강법을 했을 때 좋은 퍼포먼스가 나오지 않는다면 해결방법은

- 처음을 여러번 반복해서 다양한 가중치로 시작을 하는거야.
- 아니면 차원을 더 복잡하게 만들어서. 극소값을 줄여 거의 수렴하지 않도록 하는 거야.

---

경사하강법에서,

기울기가 0인 수준에서 멈춘다면 수렴하지 않는 경우 무한루프에 빠질 수 있어.

그래서 2가지 조건으로 종료하는 것이 가장 이상적이야. 기울기가 한계점 이하인지 아니면 epoch를 다 돌았는지.

---

gradient vanishing 에 대해서

만약 시작 지점이 기울기가 0인 부분이라면 학습을 아예 할 수가 없어.

그러나 실제에서는 굉장히 고차원이어서 이러한 일이 일어날 확률은 매우 드물어.

---

learning rate에 대해서, 초기에 크다가 학습이 진행될 수록 작아지는 것이 유리해.

이처럼 하기 위해 optimizer 같은 것을 써.

---

gradient problems, vanishing gradient와 exploding gradient가 있어.

gradient = 0 일 때 기울기 소실이 일어나고

gradient가 너무 커서 최소값을 넘어 다음 극값을 가지게 될 때, exploding gradient가 일어나

이 경우 solution값이 틀린 값을 가지겠지

어떻게 하면 이런 문제를 줄일 수 있을까?

1. 은닉층의 수를 줄인다.
2. relu같은 활성화 함수를 쓴다.
3. 가중치를 정규화한다.
4. autoencoder 로 모델을 미리 학습시킨다.
5. 배치정규화나 드랍아웃, 가중치 소실 등 가중치를 소실시키는 regularization 기술을 사용한다.
6. resnet 이라는 경사 문제가 없도록 설계된 구조를 사용할 수 있다.

 
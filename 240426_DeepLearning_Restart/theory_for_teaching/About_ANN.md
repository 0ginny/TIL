### perceptron

ann 의 기본 구조 

y = NonLinear(xTw)

위의 모델은 bias를 넣지 않은 모델이야. 이 경우 반드시 원점을 지나야 하는 한계가 있어.

그래서 수정된 구조

y = sigma(xTw + w0)

---

### loss function

그러면 이러한 가중치는 어떻게 변경할까?

역전파 방식을 이용해서 변경한는데,

역전파는 실제값과 예측값의 차이를 통해 경사하강법으로 가중치를 변경하는 방식이야.

그래서 우선 값의 차이를 알기위해 우리는 손실함수를 정해야해.

보통 크게 두 종류의 손실함수가 있어.

#### Mean-squared error (MSE)

연속데이터를 사용하면서 출력도 연속된 값일때.

L = 1/2 * (y_pred - y)^2

#### Cross-entropy (logistic error)

범주형 데이터를 써서 확률로 나타내는 함수
L = -(ylog(y_pred) + (1-y)log(1-y_pred))


### Cost function

이때 손실함수는 한 샘플링의 차이를 의미해. 

여기서 손실함수의 값의 평균을 구한것이 비용함수야.

J = 1/n * sum(L(y_pred_i,y_i)) = 1/n*sum(L(f(x,W)_i, y_i))

이러한 비용함수를 경사하강법을 적용할 기준 함수로 사용을 해.

#### 그럼 왜 손실함수가 아닌 비용함수가 기준이 되는가?

- 보든 샘플링마다 손실을 구하는 건 엄청난 시간적 과부하가 있고, 과적합을 야기할 수 있어.
- 그런데 여기서 너무 많은 샘플의 평균을 구하게 될 경우 민감도가 감소할 수 있어. 특히 표본 변동성이 클 경우
- 그래서 가장 좋은 방법은 샘플링 단위가 아니라 batch 단위로 비용함수를 구하는 거야. (미니배치경사하강법)

---

### 그럼 왜 아직도 통계적 방법이 많이 사용되는가??

사실 딥러닝은 최적의 값을 내는데에 적합하지는 않아.

그리고 수학적으로 뛰어나지도 않지

---

### back propergation

일단 이제부터 하나의 퍼셉트론을 unit(node) 으로 표시할거야.

그리고 이 노드들은 모두 독립적이야. 전체의 상황을 모르는 상태야.

그냥 셀 수 없이 많은 연산을 통해 예측을 하는 거야.

그러나 시대가 지나면 이것도 대체될 수도 있지
이 때, 역전파는 완전히 경사하강법과 동일하게 이루어져.

그런데 우리가 바꿀 수 있는건 w 뿐이라.

w <- w - lr*Loss_deriv (vanila gradient descent)

Loss_deriv = deriv(L(non_linear(xTw,y))

이때 Loss는 Cost로 변경되어 쓸 수 있어. 

#### 히든노드의 가중치는 어떻게 업데이트 되는가?

[참고 블로그](https://amber-chaeeunk.tistory.com/18)

역전파 과정은 chain rule로 진행돼. 각각 노드끼리의 값을 함수로 연결해서, 하나의 함성 함수로 만들고

그 함수를 각각 w_i로 편미분해서 값을 각 w_i값을 업데이트 하는 방식 

### 학습률

[학습률 비교 에니메이션 코드 github](https://github.com/pablocpz/Gradient-Descent-Visualizations)

[1D 학습률 확인용 수정 코드](https://drive.google.com/file/d/1JlDZC_SSpwTR9Z8JnR0D07Q9Zh0xoHcl/view?usp=drive_link)

학습률이 너무 높으면 아예 수렴하지 않을 수 있어. 계속 극소값 근처만 가고, 극소값 안으로 들어올 수 없으니.

그런데 차원이 증가해도 그럴까? 무수히 많은 차원에서는 어떻게 될진 모르겠다...

그래도 각각 수렴이 안될 거 같긴해.
일단 하나의 배치에서 (순전파, 손실 계산, 역잔파, 가중치 업데이트)의 일련의 과정을 가져

일단 배치라 함은 그냥 데이터 셋이야.

그래서 배치를 나누지 않은 경우는 N = N 이라고 생각하면 되지

배치를 나누는 경우 (N = k)

각각의 배치들은 서로 관련이 없이 배타적이야.

보통 배치 사이즈는 2의 제곱수로 많이 사용돼. (2 ~ 512) 
아니어도 괜찮지만, 이미지 같이 큰 데이터를 처리할 땐 효율성을 위해 2의 제곱수가 좋아.

** 추가적으로 batch size = 1로 학습을 하는 경우 stochastic gradient descent(SGD)

** 팁 : 만약 데이터의 유사성이 높다면, 오히려 배치를 줄이는 것이 학습속도가 빨라져

### 미니 배치 학습의 장점

1. 학습 시간을 줄일 수 있어. 
   - for 문 대신 행렬곱으로 처리할 수 있어져.
     - 그러나 배치 사이즈가 너무 크면, 행렬곱 계산이 복잡해져서 오히려 계산 시간이 오래 걸릴 수 있어.
2. 오버피팅을 막을 수 있어. 

### 비유
100개의 시험 피드백

- SGD : 각각의 모든 문제를 풀때마다 피드백을 줘. 너무 좋지만 시간이 오래 걸리겠지
- one batch : 전체 시험이 다 끝나면 결과를 보여줘. 그 전까지 변화할 수 없지만, 결과는 가장 빨리 나와
- mini-batch : 10개의 시험을 볼때마다 평균을 보여줘. 그래서 어느정도 중간부터 변화를 줄 수 있어.
  - 이런 균형을 잘 맞추면 학습률이 좋아져